{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coursework 2\n",
    "Objective:\n",
    "(1) obtain practical knowledge and hands-on understanding of the basic concepts in Generative Adversarial Nets(GAN);\n",
    "(2) obtain practical experience on how to implement basic GAN using tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks: three subtasks are involved:\n",
    "1. Coding: to add your code blocks in the required sections; (30% of this CW)\n",
    "2. Demonstrating: to answer TWO questions randomly selected from below\n",
    "during the lab demo session in WK11; (20% of this CW)\n",
    "3. Report: to complete the questions in report. (50% of this CW)\n",
    "\n",
    "Basic material:\n",
    "Some of online materials for tensorflow-code may help you better complete this coursework (if you not familiar with tensorflow, you can follow this step by step) https://www.tensorflow.org/versions/r1.1/get_started/mnist/beginners https://github.com/floydhub/tensorflow-notebooks- examples/blob/master/3_NeuralNetworks/autoencoder.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The questions to think over:\n",
    "\n",
    "**1. What are two basic part in generative model?**<br>\n",
    "* a generative model G\n",
    "* a discriminative model D \n",
    "\n",
    "**2. What is the specific objective of these two part?**<br>\n",
    "* generative model G - captures the data distribution\n",
    "* discriminative model D - estimates the probability that a sample came from the training data rather than G\n",
    "\n",
    "**3. What is the basic loss function of GAN?**<br>\n",
    "Loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. Need to minimize a loss function.\n",
    "\n",
    "The discriminator(G_sample) wants the predictions on the “real” data to be all ones and the predictions on the “fake” data from the generator to be all zeros. The generator wants the discriminator’s predictions to be all ones, as per definition, we want to maximize the probability of real data.\n",
    "\n",
    "Maximizing tf.reduce_mean(tf.log(D_fake)).\n",
    "\n",
    "**4. What is training process of basic GAN model?**<br>\n",
    "Training the GAN means Training the Generator. The generator initially produces garbage images, and the loss value is high. So, the back-propagation updates the generator’s weights to produce more realistic images as the training continues. This is how the generator is trained via training the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dask --upgrade\n",
    "import os, time, itertools, imageio, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# G(z)\n",
    "def generator(x):\n",
    "    # initializers\n",
    "    w_init = tf.truncated_normal_initializer(mean=0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.)\n",
    "    # 1st hidden layer\n",
    "    w0 = tf.get_variable('G_w0', [x.get_shape()[1], 128], initializer=w_init)\n",
    "    b0 = tf.get_variable('G_b0', [128], initializer=b_init)\n",
    "    h0 = tf.nn.relu(tf.matmul(x, w0) + b0)\n",
    "    # output hidden layer\n",
    "    w1 = tf.get_variable('G_w1', [h0.get_shape()[1], 784], initializer=w_init)\n",
    "    b1 = tf.get_variable('G_b1', [784], initializer=b_init)\n",
    "    o = tf.nn.tanh(tf.matmul(h0, w1) + b1)\n",
    "    return o\n",
    "    ### Code:ToDO( Change the architecture as CW2 Guidance required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# D(x)\n",
    "def discriminator(x, drop_out):\n",
    "    # initializers\n",
    "    w_init = tf.truncated_normal_initializer(mean=0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.)\n",
    "    # 1st hidden layer\n",
    "    w0 = tf.get_variable('D_w0', [x.get_shape()[1], 784], initializer=w_init)\n",
    "    b0 = tf.get_variable('D_b0', [784], initializer=b_init)\n",
    "    h0 = tf.nn.relu(tf.matmul(x, w0) + b0)\n",
    "    # output layer\n",
    "    w1 = tf.get_variable('D_w1', [h0.get_shape()[1], 1], initializer=w_init)\n",
    "    b1 = tf.get_variable('D_b1', [1], initializer=b_init)\n",
    "    o = tf.sigmoid(tf.matmul(h0, w1) + b1)\n",
    "    ###  Code: ToDO( Change the architecture as CW2 Guidance required)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n",
    "    z_ = np.random.normal(0, 1, (25, 100))    # z_ is the input of generator, every epochs will random produce input\n",
    "    ##Code:ToDo complete the rest of part\n",
    "\n",
    "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
    "    x = range(len(hist['D_losses']))\n",
    "\n",
    "    y1 = hist['D_losses']\n",
    "    y2 = hist['G_losses']\n",
    "\n",
    "    plt.plot(x, y1, label='D_loss')\n",
    "    plt.plot(x, y2, label='G_loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 100\n",
    "lr = 0.0002\n",
    "train_epoch = 100\n",
    "\n",
    "# load MNIST\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "train_set = (mnist.train.images - 0.5) / 0.5  # normalization; range: -1 ~ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# networks : generator\n",
    "with tf.variable_scope('G'):\n",
    "    z = tf.placeholder(tf.float32, shape=(None, 100))\n",
    "    G_z = generator(z)\n",
    "# networks : discriminator\n",
    "with tf.variable_scope('D') as scope:\n",
    "    drop_out = tf.placeholder(dtype=tf.float32, name='drop_out')\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "    D_real = discriminator(x, drop_out)\n",
    "    scope.reuse_variables()\n",
    "    D_fake = discriminator(G_z, drop_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss for each network\n",
    "eps = 1e-2\n",
    "D_loss = tf.reduce_mean(-tf.log(D_real + eps) - tf.log(1 - D_fake + eps))\n",
    "G_loss = tf.reduce_mean(-tf.log(D_fake + eps))\n",
    "\n",
    "# trainable variables for each network\n",
    "t_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in t_vars if 'D_' in var.name]\n",
    "G_vars = [var for var in t_vars if 'G_' in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer for each network\n",
    "D_optim = tf.train.AdamOptimizer(lr).minimize(D_loss, var_list=D_vars)\n",
    "G_optim = tf.train.AdamOptimizer(lr).minimize(G_loss, var_list=G_vars)\n",
    "\n",
    "# open session and initialize all variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# results save folder\n",
    "if not os.path.isdir('MNIST_GAN_results'):\n",
    "    os.mkdir('MNIST_GAN_results')\n",
    "if not os.path.isdir('MNIST_GAN_results/results'):\n",
    "    os.mkdir('MNIST_GAN_results/results')\n",
    "train_hist = {}\n",
    "train_hist['D_losses'] = []\n",
    "train_hist['G_losses'] = []\n",
    "train_hist['per_epoch_ptimes'] = []\n",
    "train_hist['total_ptime'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] - ptime: 27.36 loss_d: 0.488, loss_g: 1.061\n",
      "[2/100] - ptime: 24.40 loss_d: 0.182, loss_g: 2.142\n",
      "[3/100] - ptime: 23.63 loss_d: 0.292, loss_g: 2.458\n",
      "[4/100] - ptime: 26.08 loss_d: 0.474, loss_g: 2.243\n",
      "[5/100] - ptime: 23.52 loss_d: 0.292, loss_g: 2.518\n",
      "[6/100] - ptime: 24.28 loss_d: 0.366, loss_g: 2.410\n",
      "[7/100] - ptime: 26.22 loss_d: 0.398, loss_g: 2.278\n",
      "[8/100] - ptime: 26.58 loss_d: 0.560, loss_g: 2.186\n",
      "[9/100] - ptime: 24.71 loss_d: 0.715, loss_g: 1.925\n",
      "[10/100] - ptime: 26.58 loss_d: 0.740, loss_g: 1.823\n",
      "[11/100] - ptime: 25.05 loss_d: 0.767, loss_g: 1.623\n",
      "[12/100] - ptime: 27.54 loss_d: 0.735, loss_g: 1.754\n",
      "[13/100] - ptime: 25.09 loss_d: 0.672, loss_g: 1.928\n",
      "[14/100] - ptime: 28.52 loss_d: 0.589, loss_g: 2.055\n",
      "[15/100] - ptime: 24.77 loss_d: 0.785, loss_g: 1.666\n",
      "[16/100] - ptime: 22.83 loss_d: 0.839, loss_g: 1.740\n",
      "[17/100] - ptime: 22.00 loss_d: 0.893, loss_g: 1.567\n",
      "[18/100] - ptime: 21.53 loss_d: 0.864, loss_g: 1.563\n",
      "[19/100] - ptime: 21.60 loss_d: 0.844, loss_g: 1.568\n",
      "[20/100] - ptime: 24.19 loss_d: 0.706, loss_g: 1.746\n",
      "[21/100] - ptime: 30.03 loss_d: 0.833, loss_g: 1.616\n",
      "[22/100] - ptime: 25.85 loss_d: 0.700, loss_g: 1.749\n",
      "[23/100] - ptime: 24.27 loss_d: 0.717, loss_g: 1.815\n",
      "[24/100] - ptime: 29.32 loss_d: 0.653, loss_g: 1.957\n",
      "[25/100] - ptime: 24.36 loss_d: 0.666, loss_g: 1.948\n",
      "[26/100] - ptime: 26.99 loss_d: 0.627, loss_g: 2.012\n",
      "[27/100] - ptime: 24.95 loss_d: 0.468, loss_g: 2.281\n",
      "[28/100] - ptime: 24.75 loss_d: 0.412, loss_g: 2.415\n",
      "[29/100] - ptime: 28.63 loss_d: 0.468, loss_g: 2.382\n",
      "[30/100] - ptime: 25.31 loss_d: 0.556, loss_g: 2.374\n",
      "[31/100] - ptime: 23.71 loss_d: 0.560, loss_g: 2.264\n",
      "[32/100] - ptime: 24.34 loss_d: 0.528, loss_g: 2.283\n",
      "[33/100] - ptime: 24.11 loss_d: 0.522, loss_g: 2.405\n",
      "[34/100] - ptime: 22.97 loss_d: 0.572, loss_g: 2.346\n",
      "[35/100] - ptime: 28.45 loss_d: 0.521, loss_g: 2.481\n",
      "[36/100] - ptime: 33.48 loss_d: 0.474, loss_g: 2.526\n",
      "[37/100] - ptime: 29.79 loss_d: 0.360, loss_g: 2.769\n",
      "[38/100] - ptime: 22.53 loss_d: 0.297, loss_g: 3.064\n",
      "[39/100] - ptime: 24.99 loss_d: 0.371, loss_g: 2.871\n",
      "[40/100] - ptime: 26.48 loss_d: 0.410, loss_g: 2.892\n",
      "[41/100] - ptime: 23.77 loss_d: 0.484, loss_g: 2.809\n",
      "[42/100] - ptime: 22.42 loss_d: 0.594, loss_g: 2.677\n",
      "[43/100] - ptime: 25.41 loss_d: 0.587, loss_g: 2.710\n",
      "[44/100] - ptime: 26.61 loss_d: 0.625, loss_g: 2.660\n",
      "[45/100] - ptime: 23.50 loss_d: 0.634, loss_g: 2.691\n",
      "[46/100] - ptime: 23.79 loss_d: 0.654, loss_g: 2.666\n",
      "[47/100] - ptime: 22.57 loss_d: 0.714, loss_g: 2.632\n",
      "[48/100] - ptime: 26.83 loss_d: 0.731, loss_g: 2.475\n",
      "[49/100] - ptime: 27.05 loss_d: 0.711, loss_g: 2.452\n",
      "[50/100] - ptime: 32.03 loss_d: 0.671, loss_g: 2.564\n",
      "[51/100] - ptime: 34.65 loss_d: 0.700, loss_g: 2.561\n",
      "[52/100] - ptime: 26.01 loss_d: 0.633, loss_g: 2.674\n",
      "[53/100] - ptime: 27.31 loss_d: 0.594, loss_g: 2.810\n",
      "[54/100] - ptime: 27.61 loss_d: 0.561, loss_g: 2.876\n",
      "[55/100] - ptime: 25.61 loss_d: 0.444, loss_g: 3.036\n",
      "[56/100] - ptime: 22.66 loss_d: 0.559, loss_g: 2.955\n",
      "[57/100] - ptime: 22.86 loss_d: 0.578, loss_g: 2.880\n",
      "[58/100] - ptime: 23.68 loss_d: 0.633, loss_g: 2.786\n",
      "[59/100] - ptime: 22.61 loss_d: 0.766, loss_g: 2.637\n",
      "[60/100] - ptime: 21.74 loss_d: 0.765, loss_g: 2.676\n",
      "[61/100] - ptime: 21.65 loss_d: 0.807, loss_g: 2.607\n",
      "[62/100] - ptime: 23.82 loss_d: 0.883, loss_g: 2.453\n",
      "[63/100] - ptime: 27.97 loss_d: 0.834, loss_g: 2.485\n",
      "[64/100] - ptime: 33.38 loss_d: 0.906, loss_g: 2.420\n",
      "[65/100] - ptime: 33.46 loss_d: 0.907, loss_g: 2.486\n",
      "[66/100] - ptime: 26.24 loss_d: 0.926, loss_g: 2.481\n",
      "[67/100] - ptime: 24.26 loss_d: 0.742, loss_g: 2.683\n",
      "[68/100] - ptime: 24.51 loss_d: 0.581, loss_g: 2.877\n",
      "[69/100] - ptime: 21.84 loss_d: 0.595, loss_g: 2.950\n",
      "[70/100] - ptime: 25.88 loss_d: 0.671, loss_g: 2.927\n",
      "[71/100] - ptime: 24.81 loss_d: 0.804, loss_g: 2.737\n",
      "[72/100] - ptime: 22.23 loss_d: 0.739, loss_g: 2.709\n",
      "[73/100] - ptime: 21.64 loss_d: 0.851, loss_g: 2.553\n",
      "[74/100] - ptime: 21.69 loss_d: 0.878, loss_g: 2.552\n",
      "[75/100] - ptime: 21.62 loss_d: 0.907, loss_g: 2.457\n",
      "[76/100] - ptime: 21.67 loss_d: 0.870, loss_g: 2.455\n",
      "[77/100] - ptime: 21.60 loss_d: 0.827, loss_g: 2.515\n",
      "[78/100] - ptime: 21.63 loss_d: 0.751, loss_g: 2.589\n",
      "[79/100] - ptime: 21.61 loss_d: 0.631, loss_g: 2.807\n",
      "[80/100] - ptime: 21.88 loss_d: 0.589, loss_g: 2.912\n",
      "[81/100] - ptime: 21.80 loss_d: 0.538, loss_g: 3.047\n",
      "[82/100] - ptime: 24.58 loss_d: 0.545, loss_g: 3.097\n",
      "[83/100] - ptime: 24.50 loss_d: 0.498, loss_g: 3.177\n",
      "[84/100] - ptime: 22.07 loss_d: 0.579, loss_g: 3.133\n",
      "[85/100] - ptime: 26.54 loss_d: 0.572, loss_g: 3.088\n",
      "[86/100] - ptime: 26.34 loss_d: 0.595, loss_g: 3.005\n",
      "[87/100] - ptime: 25.77 loss_d: 0.722, loss_g: 2.863\n",
      "[88/100] - ptime: 23.18 loss_d: 0.768, loss_g: 2.784\n",
      "[89/100] - ptime: 24.08 loss_d: 0.762, loss_g: 2.752\n",
      "[90/100] - ptime: 22.89 loss_d: 0.784, loss_g: 2.694\n",
      "[91/100] - ptime: 22.19 loss_d: 0.792, loss_g: 2.653\n",
      "[92/100] - ptime: 23.02 loss_d: 0.800, loss_g: 2.635\n",
      "[93/100] - ptime: 21.82 loss_d: 0.749, loss_g: 2.691\n",
      "[94/100] - ptime: 22.11 loss_d: 0.700, loss_g: 2.800\n",
      "[95/100] - ptime: 23.49 loss_d: 0.543, loss_g: 3.020\n",
      "[96/100] - ptime: 23.76 loss_d: 0.591, loss_g: 3.044\n",
      "[97/100] - ptime: 23.81 loss_d: 0.433, loss_g: 3.198\n",
      "[98/100] - ptime: 24.60 loss_d: 0.530, loss_g: 3.151\n",
      "[99/100] - ptime: 23.26 loss_d: 0.545, loss_g: 3.151\n",
      "[100/100] - ptime: 22.50 loss_d: 0.575, loss_g: 3.076\n",
      "Avg per epoch ptime: 24.86, total 100 epochs ptime: 2486.25\n",
      "Training finish!... save training results\n"
     ]
    }
   ],
   "source": [
    "# training-loop\n",
    "np.random.seed(int(time.time()))\n",
    "start_time = time.time()\n",
    "for epoch in range(train_epoch):\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    epoch_start_time = time.time()\n",
    "    for iter in range(train_set.shape[0] // batch_size):\n",
    "        # update discriminator\n",
    "        x_ = train_set[iter*batch_size:(iter+1)*batch_size]\n",
    "        z_ = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "        loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, z: z_, drop_out: 0.3})\n",
    "        D_losses.append(loss_d_)\n",
    "\n",
    "        # update generator\n",
    "        z_ = np.random.normal(0, 1, (batch_size, 100))\n",
    "        loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, drop_out: 0.3})\n",
    "        G_losses.append(loss_g_)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "    print('[%d/%d] - ptime: %.2f loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, np.mean(D_losses), np.mean(G_losses)))\n",
    "\n",
    "    ### Code: TODO Code complet show_result function)\n",
    "    ###p = 'MNIST_GAN_results/results/MNIST_GAN_' + str(epoch + 1) + '.png'\n",
    "    ###show_result((epoch + 1), save=True, path=p)\n",
    "    train_hist['D_losses'].append(np.mean(D_losses))\n",
    "    train_hist['G_losses'].append(np.mean(G_losses))\n",
    "    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
    "end_time = time.time()\n",
    "total_ptime = end_time - start_time\n",
    "train_hist['total_ptime'].append(total_ptime)\n",
    "print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(train_hist['per_epoch_ptimes']), train_epoch, total_ptime))\n",
    "print(\"Training finish!... save training results\")\n",
    "with open('MNIST_GAN_results/train_hist.pkl', 'wb') as f:\n",
    "    pickle.dump(train_hist, f)\n",
    "show_train_hist(train_hist, save=True, path='MNIST_GAN_results/MNIST_GAN_train_hist.png')\n",
    "images = []\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The excises to conduct:\n",
    "\n",
    "**1. Understanding the  framework.**<br>\n",
    "1) Dataset: What is dataset being used? How can you load this dataset?\n",
    "#MNIST dataset was used\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "2) Model: Can you plot/draw the basic architecture in this case?\n",
    "Bellow\n",
    "3) Loss: What is loss function in this example?\n",
    "eps = 1e-2\n",
    "D_loss = tf.reduce_mean(-tf.log(D_real + eps) - tf.log(1 - D_fake + eps))\n",
    "G_loss = tf.reduce_mean(-tf.log(D_fake + eps))\n",
    "\n",
    "4) Training: How is this network being trained?\n",
    "\n",
    "5) Test: How do you test this model?\n",
    "G learns to make data that is indistiguishable from real data to the discriminator.\n",
    "\n",
    "**2. Change the learning rate to 0.01 and train for a few epochs to understand\n",
    "how learning rate will influence the model outcome.**<br>\n",
    "Initial learning rate was lr = 0.0002 :\n",
    "[100/100] - ptime: 22.50 loss_d: 0.575, loss_g: 3.076\n",
    "Avg per epoch ptime: 4.09, total 100 epochs ptime: 409.31\n",
    "\n",
    "\n",
    "**3. Change the batch size to 256, and discuss how the batch size influence the\n",
    "model performance regarding its training speed and test accuracy.**<br>\n",
    "Loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. Need to minimize a loss function.\n",
    "\n",
    "**4. Change the training epoch to 200, run it.**<br>\n",
    "Training the GAN me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model: plot/draw the basic architecture\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img=mpimg.imread('arch.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
